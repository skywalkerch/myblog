# 主成分分析：Pricipal Component Analysis(PCA)
> 主成分分析是以一种降维算法，它能将多个指标转换为少数几个主成分，这些主成分是原始变量的线性组合，且彼此之间互不相关，其能反映出原始数据的大部分信息。一般来说，当研究问题涉及到多变量且变量之间存在很强的相关性使，我们可以考虑使用主成分分析的方法对数据进行简化。

## 问题的提出
在实际问题研究中，多变量问题是经常遇到的，变量太多，无疑会增加分析问题的难度与复杂性，而且在许多实际问题中，多个变量之间是具有一定的相关关系的。
因此，需要用较少的新变量代替原来较多的旧变量，而且使这些新的变量尽可能多的保留原来变量所反映的信息。主成分分析法就是处理这种问题的一个工具（主成分分析是把原来多个变量划为少数几个综合指标的一种统计分析方法，从数学角度来看，这是一种降维处理技术）
## 主成分分析的思想
假设有$n$个评价样本，$p$个评价指标，则可以构成大小为$n \times p$的样本矩阵$x$：
$$x=\begin{align}\left[ \begin{array}{c}
	x_{11}&x_{12}&\cdots& x_{1p}\\
	x_{21}& x_{22}& \cdots&  x_{2p}\\
	\vdots & \vdots & \ddots & \vdots\\
	x_{n1}& x_{n2}& \cdots&  x_{np}\\
\end{array} \right] =(x_1,x_2,\cdots,x_p)\end{align}$$
列向量:
$$
x_1=\left[\begin{matrix}
x_{11}\\
x_{21}\\
\vdots\\
x_{n1}\\
\end{matrix}\right]
$$


$$
x_2=\left[\begin{matrix}
x_{12}\\
x_{22}\\
\vdots\\
x_{n2}\\
\end{matrix}\right]
$$


$$
x_p=\left[\begin{matrix}
x_{1p}\\
x_{2p}\\
\vdots\\
x_{np}\\
\end{matrix}\right]
$$
我们要找到新的一组变量$z_1,z_2,\cdots,z_m(m \leq p)$,且他们满足
$$
\begin{align}
\left\{ \begin{array}{c}
	z_1=l_{11}x_1+l_{12}x_2+\cdots l_{1p}x_p\\
	z_2=l_{21}x_1+l_{22}x_2+\cdots l_{2p}x_p\\
\vdots \\
	z_m=l_{m1}x_1+l_{m2}x_2+\cdots l_{mp}x_p\\
\end{array} \right.
\end{align}
$$
## PCA计算步骤
1. 首先对样本矩阵进行标准化处理
> 按列计算均值$\overline{x_j}=\frac{1}{n}\sum^{n}_{i=1}x_{ij}$和标准差$S_j=\sqrt{\frac{\sum^{n}_{i=1}(x_{ij}-\overline{x_{j}})^2}{n-1}}$,计算得到标准化数据$X_{ij}=\frac{x_{ij}\overline{-x_{j}}}{S_j}$，原始样本矩阵经过标准化变为
> $$
\begin{align}
X=\left[\begin{matrix}
X_{11}&X_{12}&\cdots&X_{1p}\\
X_{21}&X_{22}&\cdots&X_{2p}\\
\vdots&\vdots&\ddots&\vdots&\\
X_{n1}&X_{n2}&\cdots&X_{np}
&\end{matrix}\right]=(X_1,X_2,\cdots,X_p)
\end{align}
$$

2.计算标准化之后的样本矩阵的协方差矩阵
$$\begin{align}
R=\left[\begin{matrix}
r_{11}&r_{12}&\cdots&r_{1p}\\
r_{21}&r_{22}&\cdots&r_{2p}\\
\vdots&\vdots&\ddots&\cdots&\\
r_{n1}&r_{n2}&\cdots&r_{np}
&\end{matrix}\right]=(X_1,X_2,\cdots,X_p)
\end{align}$$
> 其中$r_{ij}=\frac{1}{n-1}\sum^{n}_{k=1}(X_{ki}-\overline{X_{i}})(X_{kj}-\overline{X_{j}})=\frac{1}{n-1}\sum^{n}_{k=1}X_{ki}X_{kj}=\frac{X^{T}X}{n-1}$

- 上面两部可以简化为直接求最初的样本矩阵x的相关系数矩阵
$$\begin{align}
r_{ij}=\frac{\sum^{n}_{k=1}(x_{ki}-\overline{x_{i}})(x_{kj}-\overline{x_j})}{\sqrt{\sum^{n}_{k=1}(x_{ki}-\overline{x_{i}})^2\sum^{n}_{k=1}(x_{kj}-\overline{x_j})^2}}
\end{align}$$
3. 计算R的特征值和特征向量
特征值:$\lambda_1\geq\lambda_2\geq\cdots\lambda_p\geq0$(R是半正定矩阵（证明略），且$\mathcal{trace}(R)=\sum^{p}_{k=1}\lambda_k=p$)

特征向量：
$$\begin{align}
a_{1}=\left[\begin{array}{c}
a_{11} \\
a_{21} \\
\vdots \\
a_{p 1}
\end{array}\right], a_{2}=\left[\begin{array}{c}
a_{12} \\
a_{22} \\
\vdots \\
a_{p 2}
\end{array}\right], \cdots, a_{p}=\left[\begin{array}{c}
a_{1 p} \\
a_{2 p} \\
\vdots \\
a_{p p}
\end{array}\right]
\end{align}$$
mathlab函数eig(R)
- R是半正定矩阵，所以其特征值不为负数
- R同时是对称矩阵，Matlab计算对称矩阵时，会将特征值按照从小到大排列
- `[V,D] = eig(R);  % V 特征向量矩阵  D 特征值构成的对角矩阵`
4. 计算主成分贡献率以及累计贡献率
   $$\begin{align}
   \mathcal{贡献率}=\frac{\lambda_i}{\sum^{p}_{k=1}\lambda_k}(i=1,2,\cdots,p)
   \end{align}
$$
$$\begin{align}
\mathcal{累计贡献率}=\frac{\sum^{i}_{k=1}\lambda_k}{\sum^{p}_{k=1}\lambda_k}(i=1,2,\cdots,p)
\end{align}
$$
5. 写出主成分
一般取累计贡献率超过80%的特征值所对应的第一，第二，...第m($m\leq p$)个主成分。
第i个主成分：$$
F_{i}=a_{1 i} X_{1}+a_{2 i} X_{2}+\cdots+a_{p i} X_{p} \quad(i=1,2, \cdots, m)
$$
6. 根据系数分析主成分代表的意义
 对于某个主成分而言，指标前面的系数越大，代表该指标对于该主成分的影响越大。
7. 利用主成分的结果进行后续的分析
- 主成分得分：**不可用**于评价类模型
- 主成分可用于聚类分析
- 主成分可用于回归分析

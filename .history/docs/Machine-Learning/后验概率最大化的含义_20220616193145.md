---
title: 后验概率最大化的含义解读
---

最近在啃《统计学习方法》，说实话，我一个土木专业的大二学生的数学水平想要顺利吃透这本机器学习经典非常有难度，昨天看完了第四章“**朴素贝叶斯的学习方法与分类**”，并存在一个问题——[**后验概率最大化的详细证明**]。今天在网上看到好多博客对这个地方做了详细的解释，我也在这里写下我自己的见解

## 0-1损失函数

$$
L(Y,f(X))=\left\{\begin{aligned}
1, \ Y\neq f(X)\\
0, \ Y=f(X)

\end{aligned} \right .
$$

-  $f(X)$是分类决策函数，即习得的贝叶斯模型

  此时期望风险函数表示为$R_{exp}(f)=E[L(Y,f(X))]$

因为期望是该值与该值出现的概率**之积**，而该损失函数又是基于$X,Y$的联合分布所设的，因此损失函数的期望也可以表示为
$$
R_{exp}(f)=\sum\sum L(Y,f(X)) P(X,Y)
$$
那么就有
$$
R_{exp}(f)=\sum \sum L(Y,f(X))P(Y|X)P(X)
$$

> 这里需要说明两个求和符号的含义
>
> - 网上很多博客这里都是用的二重积分，但二重积分仅适用于特征向量$x$和类标记$y$连续的情况,而实际情况下很多时候这些数据都是离散的，因此我在这里用的是求和符号
>
> - 第一个求和符号表示对于$y \in \mathcal{Y},y \in \{c_1,\ldots c_k\}$ ,在$k$种**类标记**上进行求和
>
> - 在每一种类标记（即$y$的取值）下,第二个求和符号是对于$x \in \mathcal{X}$,$x=\{(x^{(1)}_{1} ,\ldots, x^{(n)}_{1}),\ldots,(x^{(1)}_{N} ,\ldots, x^{(n)}_{N}) \}$,对所有的**特征向量**进行求和

而对于特征空间而言$P(X=x)$为**常数**，损失函数$\geq 0$, $P(Y|X) \geq 0$，因此为了使$R_{exp}$最小化，只需对$X=x$逐个进行最小化

即对于函数进行最小化


$$
\begin{align*}
  f(x) &=\mathop{arg} \min_{y \in \mathcal{Y}}\sum_{k=1}^{K}L(c_k,f(X))P(c_k|X=x)  \\
    &= \mathop{arg} \min_{y \in \mathcal{Y}}\sum_{k=1}^{K}P(y \neq c_k|X=x) \\
    &= \mathop{arg} \min_{y \in \mathcal{Y}}\sum_{k=1}^{K}(1-P(y = c_k|X=x))\\
    &=\mathop{arg} \max_{y \in \mathcal{Y}}\sum_{k=1}^{K}P(y = c_k|X=x)
\end{align*}
$$
这样一来就推出了后验概率最大化的公式
$$
f(x)=\mathop{arg} \max_{y \in \mathcal{Y}}\sum_{k=1}^{K}P(y = c_k|X=x)
$$


> 数学真是太神奇了
